{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Split into training (80%) and validation (20%)."
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Remove Gutenberg header/footer\n",
        "start = text.find(\"*** START OF\")\n",
        "end = text.find(\"*** END OF\")\n",
        "text = text[start:end]\n",
        "\n",
        "# Split into training (80%) and validation (20%)\n",
        "split_index = int(len(text) * 0.8)\n",
        "train_text = text[:split_index]\n",
        "val_text = text[split_index:]"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation.replace('.', '').replace('!', '').replace('?', ''))}]\", \"\", text)\n",
        "    return text.split()\n",
        "\n",
        "train_words = preprocess(train_text)\n",
        "val_words = preprocess(val_text)\n",
        "\n",
        "# Vocabulary\n",
        "vocab = sorted(set(train_words + val_words))\n",
        "word_to_id = {word: i for i, word in enumerate(vocab)}\n",
        "id_to_word = {i: word for word, i in word_to_id.items()}\n",
        "\n",
        "# Encode to IDs\n",
        "train_ids = [word_to_id[word] for word in train_words]\n",
        "val_ids = [word_to_id[word] for word in val_words]\n"
      ],
      "metadata": {
        "id": "RvXRFVcbMLe9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "sequence_length = 20\n",
        "embedding_dim = 128\n",
        "\n",
        "#Sequence preparation\n",
        "import numpy as np\n",
        "\n",
        "def create_sequences(data):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i+sequence_length])\n",
        "        y.append(data[i+sequence_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_train, y_train = create_sequences(train_ids)\n",
        "X_val, y_val = create_sequences(val_ids)\n"
      ],
      "metadata": {
        "id": "OXCK40l6MRld"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
      ],
      "metadata": {
        "id": "linweGaUMg0T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible. If you have higher value (which is possible) try to draw conclusions, why doesn't it decrease to a lower value."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# Calculate perplexity\n",
        "def perplexity(loss):\n",
        "    return math.exp(loss)\n",
        "\n",
        "train_loss = history.history['loss'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\nTraining Perplexity: {perplexity(train_loss):.2f}\")\n",
        "print(f\"Validation Perplexity: {perplexity(val_loss):.2f}\")\n"
      ],
      "metadata": {
        "id": "P8d8FS2XMj46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa4c86d-0be4-4a01-d188-1e8a717b5fac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 299ms/step - accuracy: 0.1561 - loss: 4.9545 - val_accuracy: 0.1333 - val_loss: 5.7728\n",
            "Epoch 2/5\n",
            "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 298ms/step - accuracy: 0.1740 - loss: 4.6563 - val_accuracy: 0.1358 - val_loss: 5.8014\n",
            "Epoch 3/5\n",
            "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 298ms/step - accuracy: 0.1932 - loss: 4.3662 - val_accuracy: 0.1350 - val_loss: 5.8638\n",
            "Epoch 4/5\n",
            "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 299ms/step - accuracy: 0.2102 - loss: 4.1101 - val_accuracy: 0.1345 - val_loss: 5.9431\n",
            "\n",
            "Training Perplexity: 64.37\n",
            "Validation Perplexity: 381.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my case, the validation perplexity is significantly higher than 50 (around 381.10), while the training perplexity is around 64.37. This large gap suggests that the model is overfitting to the training data and struggling to generalize to unseen text.\n",
        "There could be multiple possible reasons. It's maybe because the vocabulary size is large due to the complexity and richness of the literary text used, which increases prediction difficulty. Or the sequence length may be too short (20 tokens) to capture long-term dependencies in the language. It also could be the model capacity (a single LSTM layer) might be insufficient for the complexity of the dataset. Training may also require more epochs or a larger dataset to improve generalization.\n",
        "To improve performance and reduce validation perplexity, in the future I would consider limiting the vocabulary, increasing model depth or sequence length, and potentially training on a larger or simpler dataset."
      ],
      "metadata": {
        "id": "P5N6l7p-bety"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Generate text\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_text(seed_text, num_words=50):\n",
        "    result = seed_text.lower().split()\n",
        "    for _ in range(num_words):\n",
        "        input_seq = [word_to_id.get(w, 0) for w in result[-sequence_length:]]\n",
        "        input_seq = pad_sequences([input_seq], maxlen=sequence_length)\n",
        "        predicted_probs = model.predict(input_seq, verbose=0)[0]\n",
        "        next_id = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "        result.append(id_to_word[next_id])\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Sample outputs\n",
        "print(\"\\n--- Sample 1 ---\")\n",
        "print(generate_text(\"love is\"))\n",
        "\n",
        "print(\"\\n--- Sample 2 ---\")\n",
        "print(generate_text(\"time will\"))"
      ],
      "metadata": {
        "id": "1uHjn6aHMW5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5c9a95-9e6a-4deb-818a-340d2a6d1624"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 1 ---\n",
            "love is hurt it might be still that truly continued asserted out of themselves. she delightful done explain if they and added in his morning to the doorway of examination for state.” his dear comfortably disposition for her father are dead. my very wits will safely speak “he is very lvi. a\n",
            "\n",
            "--- Sample 2 ---\n",
            "time will taste to us from she in! prefaced his church after many delay. which examine yes. threeandtwenty! so just as you means that music cover as you have always pretty certain my own brother you left elizabeth so well by him before. it is as you are exist cried she. unhesitatingly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5CpdqF9MoPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}